name: bedrock-runtime
description: Amazon Bedrock Runtime
commands:
    - name: converse-stream
      description: Sends messages to the specified Amazon Bedrock model and returns the response in a stream.
      flags:
        --additional-model-request-fields=: Additional inference parameters that the model supports, beyond the base set of inference parameters that `Converse` and `ConverseStream` support in the `inferenceConfig` field.
        --additional-model-response-field-paths=: Additional model parameters field paths to return in the response.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --guardrail-config=: Configuration information for a guardrail that you want to use in the request.
        --inference-config=: Inference parameters to pass to the model.
        --messages=: The messages that you want to send to the model.
        --model-id=!: Specifies the model or throughput with which to run inference, or the prompt resource to use in inference.
        --performance-config=: Model performance settings for the request.
        --prompt-variables=: Contains a map of variables in a prompt from Prompt management to objects containing the values to fill in for them when running model invocation.
        --request-metadata=: Key-value pairs that you can use to filter invocation logs.
        --service-tier=: Specifies the processing tier configuration used for serving the request.
        --system=: A prompt that provides instructions or context to the model about the task it should perform, or the persona it should adopt during the conversation.
        --tool-config=: Configuration information for the tools that the model can use when generating a response.
      documentation:
        command: |-
            Sends messages to the specified Amazon Bedrock model and returns the response in a stream. `ConverseStream` provides a consistent API that works with all Amazon Bedrock models that support messages. This allows you to write code once and use it with different models. Should a model have unique inference parameters, you can also pass those unique parameters to the model.

            To find out if a model supports streaming, call [GetFoundationModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GetFoundationModel.html) and check the `responseStreamingSupported` field in the response.

            The CLI doesn't support streaming operations in Amazon Bedrock, including `ConverseStream`.

            Amazon Bedrock doesn't store any text, images, or documents that you provide as content. The data is only used to generate the response.

            You can submit a prompt by including it in the `messages` field, specifying the `modelId` of a foundation model or inference profile to run inference on it, and including any other fields that are relevant to your use case.

            You can also submit a prompt from Prompt management by specifying the ARN of the prompt version and including a map of variables to values in the `promptVariables` field. You can append more messages to the prompt by using the `messages` field. If you use a prompt from Prompt management, you can't include the following fields in the request: `additionalModelRequestFields`, `inferenceConfig`, `system`, or `toolConfig`. Instead, these fields must be defined through Prompt management. For more information, see [Use a prompt from Prompt management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management-use.html).

            For information about the Converse API, see *Use the Converse API* in the *Amazon Bedrock User Guide*. To use a guardrail, see *Use a guardrail with the Converse API* in the *Amazon Bedrock User Guide*. To use a tool with a model, see *Tool use (Function calling)* in the *Amazon Bedrock User Guide*

            For example code, see *Conversation streaming example* in the *Amazon Bedrock User Guide*.

            This operation requires permission for the `bedrock:InvokeModelWithResponseStream` action.

            To deny all inference access to resources that you specify in the modelId field, you need to deny access to the `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` actions. Doing this also denies access to the resource through the base inference actions ([InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html)). For more information see [Deny access for inference on specific models](https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-deny-inference).

            For troubleshooting some of the common errors you might encounter when using the `ConverseStream` API, see [Troubleshooting Amazon Bedrock API Error Codes](https://docs.aws.amazon.com/bedrock/latest/userguide/troubleshooting-api-error-codes.html) in the Amazon Bedrock User Guide
        flag:
            additional-model-request-fields: Additional inference parameters that the model supports, beyond the base set of inference parameters that `Converse` and `ConverseStream` support in the `inferenceConfig` field. For more information, see [Model parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
            additional-model-response-field-paths: |-
                Additional model parameters field paths to return in the response. `Converse` and `ConverseStream` return the requested fields as a JSON Pointer object in the `additionalModelResponseFields` field. The following is example JSON for `additionalModelResponseFieldPaths`.

                `[ "/stop_sequence" ]`

                For information about the JSON Pointer syntax, see the [Internet Engineering Task Force (IETF)](https://datatracker.ietf.org/doc/html/rfc6901) documentation.

                `Converse` and `ConverseStream` reject an empty JSON Pointer or incorrectly structured JSON Pointer with a `400` error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by `Converse`.
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            guardrail-config: Configuration information for a guardrail that you want to use in the request. If you include `guardContent` blocks in the `content` field in the `messages` field, the guardrail operates only on those messages. If you include no `guardContent` blocks, the guardrail operates on all messages in the request body and in any included prompt resource.
            inference-config: Inference parameters to pass to the model. `Converse` and `ConverseStream` support a base set of inference parameters. If you need to pass additional parameters that the model supports, use the `additionalModelRequestFields` request field.
            messages: The messages that you want to send to the model.
            model-id: |-
                Specifies the model or throughput with which to run inference, or the prompt resource to use in inference. The value depends on the resource that you use:

                - If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see [Amazon Bedrock base model IDs (on-demand throughput)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns) in the Amazon Bedrock User Guide.
                - If you use an inference profile, specify the inference profile ID or its ARN. For a list of inference profile IDs, see [Supported Regions and models for cross-region inference](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html) in the Amazon Bedrock User Guide.
                - If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see [Run inference using a Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html) in the Amazon Bedrock User Guide.
                - If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see [Use a custom model in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html) in the Amazon Bedrock User Guide.
                - To include a prompt that was defined in [Prompt management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html), specify the ARN of the prompt version to use.

                The Converse API doesn't support [imported models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).
            performance-config: Model performance settings for the request.
            prompt-variables: Contains a map of variables in a prompt from Prompt management to objects containing the values to fill in for them when running model invocation. This field is ignored if you don't specify a prompt resource in the `modelId` field.
            request-metadata: Key-value pairs that you can use to filter invocation logs.
            service-tier: Specifies the processing tier configuration used for serving the request.
            system: A prompt that provides instructions or context to the model about the task it should perform, or the persona it should adopt during the conversation.
            tool-config: |-
                Configuration information for the tools that the model can use when generating a response.

                For information about models that support streaming tool use, see [Supported models and model features](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).
    - name: count-tokens
      description: Returns the token count for a given inference request.
      flags:
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --input=!: The input for which to count tokens.
        --model-id=!: The unique identifier or ARN of the foundation model to use for token counting.
      documentation:
        command: |-
            Returns the token count for a given inference request. This operation helps you estimate token usage before sending requests to foundation models by returning the token count that would be used if the same input were sent to the model in an inference request.

            Token counting is model-specific because different models use different tokenization strategies. The token count returned by this operation will match the token count that would be charged if the same input were sent to the model in an `InvokeModel` or `Converse` request.

            You can use this operation to:

            - Estimate costs before sending inference requests.
            - Optimize prompts to fit within token limits.
            - Plan for token usage in your applications.

            This operation accepts the same input formats as `InvokeModel` and `Converse`, allowing you to count tokens for both raw text inputs and structured conversation formats.

            The following operations are related to `CountTokens`:

            - [InvokeModel](https://docs.aws.amazon.com/bedrock/latest/API/API_runtime_InvokeModel.html) - Sends inference requests to foundation models
            - [Converse](https://docs.aws.amazon.com/bedrock/latest/API/API_runtime_Converse.html) - Sends conversation-based inference requests to foundation models
        flag:
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            input: |-
                The input for which to count tokens. The structure of this parameter depends on whether you're counting tokens for an `InvokeModel` or `Converse` request:

                - For `InvokeModel` requests, provide the request body in the `invokeModel` field
                - For `Converse` requests, provide the messages and system content in the `converse` field

                The input format must be compatible with the model specified in the `modelId` parameter.
            model-id: The unique identifier or ARN of the foundation model to use for token counting. Each model processes tokens differently, so the token count is specific to the model you specify.
    - name: get-async-invoke
      description: Retrieve information about an asynchronous invocation.
      flags:
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --invocation-arn=!: The invocation's ARN.
      documentation:
        command: Retrieve information about an asynchronous invocation.
        flag:
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            invocation-arn: The invocation's ARN.
    - name: invoke-model
      description: Invokes the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body.
      flags:
        --accept=: The desired MIME type of the inference body in the response.
        --body=: The prompt and inference parameters in the format specified in the `contentType` in the header.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content-type=: The MIME type of the input data in the request.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --guardrail-identifier=: The unique identifier of the guardrail that you want to use.
        --guardrail-version=: The version number for the guardrail.
        --model-id=!: The unique identifier of the model to invoke to run inference.
        --performance-config-latency=: Model performance settings for the request.
        --service-tier=: Specifies the processing tier type used for serving the request.
        --trace=: Specifies whether to enable or disable the Bedrock trace.
      completion:
        flag:
            performance-config-latency:
                - standard
                - optimized
            service-tier:
                - priority
                - default
                - flex
            trace:
                - ENABLED
                - DISABLED
                - ENABLED_FULL
      documentation:
        command: |-
            Invokes the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. You use model inference to generate text, images, and embeddings.

            For example code, see *Invoke model code examples* in the *Amazon Bedrock User Guide*.

            This operation requires permission for the `bedrock:InvokeModel` action.

            To deny all inference access to resources that you specify in the modelId field, you need to deny access to the `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` actions. Doing this also denies access to the resource through the Converse API actions ([Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [ConverseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html)). For more information see [Deny access for inference on specific models](https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-deny-inference).

            For troubleshooting some of the common errors you might encounter when using the `InvokeModel` API, see [Troubleshooting Amazon Bedrock API Error Codes](https://docs.aws.amazon.com/bedrock/latest/userguide/troubleshooting-api-error-codes.html) in the Amazon Bedrock User Guide
        flag:
            accept: The desired MIME type of the inference body in the response. The default value is `application/json`.
            body: The prompt and inference parameters in the format specified in the `contentType` in the header. You must provide the body in JSON format. To see the format and content of the request and response bodies for different models, refer to [Inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html). For more information, see [Run inference](https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run.html) in the Bedrock User Guide.
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            content-type: The MIME type of the input data in the request. You must specify `application/json`.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            guardrail-identifier: |-
                The unique identifier of the guardrail that you want to use. If you don't provide a value, no guardrail is applied to the invocation.

                An error will be thrown in the following situations.

                - You don't provide a guardrail identifier but you specify the `amazon-bedrock-guardrailConfig` field in the request body.
                - You enable the guardrail but the `contentType` isn't `application/json`.
                - You provide a guardrail identifier, but `guardrailVersion` isn't specified.
            guardrail-version: The version number for the guardrail. The value can also be `DRAFT`.
            model-id: |-
                The unique identifier of the model to invoke to run inference.

                The `modelId` to provide depends on the type of model or throughput that you use:

                - If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see [Amazon Bedrock base model IDs (on-demand throughput)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns) in the Amazon Bedrock User Guide.
                - If you use an inference profile, specify the inference profile ID or its ARN. For a list of inference profile IDs, see [Supported Regions and models for cross-region inference](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html) in the Amazon Bedrock User Guide.
                - If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see [Run inference using a Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html) in the Amazon Bedrock User Guide.
                - If you use a custom model, specify the ARN of the custom model deployment (for on-demand inference) or the ARN of your provisioned model (for Provisioned Throughput). For more information, see [Use a custom model in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html) in the Amazon Bedrock User Guide.
                - If you use an [imported model](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html), specify the ARN of the imported model. You can get the model ARN from a successful call to [CreateModelImportJob](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelImportJob.html) or from the Imported models page in the Amazon Bedrock console.
            performance-config-latency: Model performance settings for the request.
            service-tier: Specifies the processing tier type used for serving the request.
            trace: Specifies whether to enable or disable the Bedrock trace. If enabled, you can see the full Bedrock trace.
    - name: invoke-model-with-bidirectional-stream
      description: Invoke the specified Amazon Bedrock model to run inference using the bidirectional stream.
      flags:
        --body=!: The prompt and inference parameters in the format specified in the `BidirectionalInputPayloadPart` in the header.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --model-id=!: The model ID or ARN of the model ID to use.
      documentation:
        command: |-
            Invoke the specified Amazon Bedrock model to run inference using the bidirectional stream. The response is returned in a stream that remains open for 8 minutes. A single session can contain multiple prompts and responses from the model. The prompts to the model are provided as audio files and the model's responses are spoken back to the user and transcribed.

            It is possible for users to interrupt the model's response with a new prompt, which will halt the response speech. The model will retain contextual awareness of the conversation while pivoting to respond to the new prompt.
        flag:
            body: The prompt and inference parameters in the format specified in the `BidirectionalInputPayloadPart` in the header. You must provide the body in JSON format. To see the format and content of the request and response bodies for different models, refer to [Inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html). For more information, see [Run inference](https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run.html) in the Bedrock User Guide.
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            model-id: The model ID or ARN of the model ID to use. Currently, only `amazon.nova-sonic-v1:0` is supported.
    - name: invoke-model-with-response-stream
      description: Invoke the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body.
      flags:
        --accept=: The desired MIME type of the inference body in the response.
        --body=: The prompt and inference parameters in the format specified in the `contentType` in the header.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content-type=: The MIME type of the input data in the request.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --guardrail-identifier=: The unique identifier of the guardrail that you want to use.
        --guardrail-version=: The version number for the guardrail.
        --model-id=!: The unique identifier of the model to invoke to run inference.
        --performance-config-latency=: Model performance settings for the request.
        --service-tier=: Specifies the processing tier type used for serving the request.
        --trace=: Specifies whether to enable or disable the Bedrock trace.
      completion:
        flag:
            performance-config-latency:
                - standard
                - optimized
            service-tier:
                - priority
                - default
                - flex
            trace:
                - ENABLED
                - DISABLED
                - ENABLED_FULL
      documentation:
        command: |-
            Invoke the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. The response is returned in a stream.

            To see if a model supports streaming, call [GetFoundationModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GetFoundationModel.html) and check the `responseStreamingSupported` field in the response.

            The CLI doesn't support streaming operations in Amazon Bedrock, including `InvokeModelWithResponseStream`.

            For example code, see *Invoke model with streaming code example* in the *Amazon Bedrock User Guide*.

            This operation requires permissions to perform the `bedrock:InvokeModelWithResponseStream` action.

            To deny all inference access to resources that you specify in the modelId field, you need to deny access to the `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` actions. Doing this also denies access to the resource through the Converse API actions ([Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [ConverseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html)). For more information see [Deny access for inference on specific models](https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-deny-inference).

            For troubleshooting some of the common errors you might encounter when using the `InvokeModelWithResponseStream` API, see [Troubleshooting Amazon Bedrock API Error Codes](https://docs.aws.amazon.com/bedrock/latest/userguide/troubleshooting-api-error-codes.html) in the Amazon Bedrock User Guide
        flag:
            accept: The desired MIME type of the inference body in the response. The default value is `application/json`.
            body: The prompt and inference parameters in the format specified in the `contentType` in the header. You must provide the body in JSON format. To see the format and content of the request and response bodies for different models, refer to [Inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html). For more information, see [Run inference](https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run.html) in the Bedrock User Guide.
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            content-type: The MIME type of the input data in the request. You must specify `application/json`.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            guardrail-identifier: |-
                The unique identifier of the guardrail that you want to use. If you don't provide a value, no guardrail is applied to the invocation.

                An error is thrown in the following situations.

                - You don't provide a guardrail identifier but you specify the `amazon-bedrock-guardrailConfig` field in the request body.
                - You enable the guardrail but the `contentType` isn't `application/json`.
                - You provide a guardrail identifier, but `guardrailVersion` isn't specified.
            guardrail-version: The version number for the guardrail. The value can also be `DRAFT`.
            model-id: |-
                The unique identifier of the model to invoke to run inference.

                The `modelId` to provide depends on the type of model or throughput that you use:

                - If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see [Amazon Bedrock base model IDs (on-demand throughput)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns) in the Amazon Bedrock User Guide.
                - If you use an inference profile, specify the inference profile ID or its ARN. For a list of inference profile IDs, see [Supported Regions and models for cross-region inference](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html) in the Amazon Bedrock User Guide.
                - If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see [Run inference using a Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html) in the Amazon Bedrock User Guide.
                - If you use a custom model, specify the ARN of the custom model deployment (for on-demand inference) or the ARN of your provisioned model (for Provisioned Throughput). For more information, see [Use a custom model in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html) in the Amazon Bedrock User Guide.
                - If you use an [imported model](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html), specify the ARN of the imported model. You can get the model ARN from a successful call to [CreateModelImportJob](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelImportJob.html) or from the Imported models page in the Amazon Bedrock console.
            performance-config-latency: Model performance settings for the request.
            service-tier: Specifies the processing tier type used for serving the request.
            trace: Specifies whether to enable or disable the Bedrock trace. If enabled, you can see the full Bedrock trace.
    - name: list-async-invokes
      description: Lists asynchronous invocations.
      flags:
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --max-items=: The  total number of items to return in the command's output.
        --max-results=: The maximum number of invocations to return in one page of results.
        --next-token=: Specify the pagination token from a previous request to retrieve the next page of results.
        --page-size=: The size of each page to get in the AWS service call.
        --sort-by=: How to sort the response.
        --sort-order=: The sorting order for the response.
        --starting-token=: A token to specify where to start paginating.
        --status-equals=: Filter invocations by status.
        --submit-time-after=: Include invocations submitted after this time.
        --submit-time-before=: Include invocations submitted before this time.
      completion:
        flag:
            sort-by:
                - SubmissionTime
            sort-order:
                - Ascending
                - Descending
            status-equals:
                - InProgress
                - Completed
                - Failed
      documentation:
        command: Lists asynchronous invocations.
        flag:
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            max-items: |-
                The total number of items to return in the command's output.
                If the total number of items available is more than the value specified, a NextToken is provided in the command's output.
                To resume pagination, provide the NextToken value in the starting-token argument of a subsequent  command.
                Do not use the NextToken response element directly outside of the AWS CLI.

                For usage examples, see Pagination in the AWS Command Line Interface User Guide.
            max-results: The maximum number of invocations to return in one page of results.
            next-token: Specify the pagination token from a previous request to retrieve the next page of results.
            page-size: |-
                The size of each page to get in the AWS service call.
                This does not affect the number of items returned in the command's output.
                Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call.
                This can help prevent the AWS service calls from timing out.

                For usage examples, see Pagination in the AWS Command Line Interface User Guide.
            sort-by: How to sort the response.
            sort-order: The sorting order for the response.
            starting-token: |-
                A token to specify where to start paginating.
                This is the NextToken from a previously truncated response.

                For usage examples, see Pagination in the AWS Command Line Interface User Guide.
            status-equals: Filter invocations by status.
            submit-time-after: Include invocations submitted after this time.
            submit-time-before: Include invocations submitted before this time.
    - name: start-async-invoke
      description: Starts an asynchronous invocation.
      flags:
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --client-request-token=: Specify idempotency token to ensure that requests are not duplicated.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --model-id=!: The model to invoke.
        --model-input=!: Input to send to the model.
        --output-data-config=!: Where to store the output.
        --tags=: Tags to apply to the invocation.
      documentation:
        command: |-
            Starts an asynchronous invocation.

            This operation requires permission for the `bedrock:InvokeModel` action.

            To deny all inference access to resources that you specify in the modelId field, you need to deny access to the `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` actions. Doing this also denies access to the resource through the Converse API actions ([Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [ConverseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html)). For more information see [Deny access for inference on specific models](https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-deny-inference).
        flag:
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            client-request-token: Specify idempotency token to ensure that requests are not duplicated.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            model-id: The model to invoke.
            model-input: Input to send to the model.
            output-data-config: Where to store the output.
            tags: Tags to apply to the invocation.
    - name: apply-guardrail
      description: The action to apply a guardrail.
      flags:
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content=!: The content details used in the request to apply the guardrail.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --guardrail-identifier=!: The guardrail identifier used in the request to apply the guardrail.
        --guardrail-version=!: The guardrail version used in the request to apply the guardrail.
        --output-scope=: Specifies the scope of the output that you get in the response.
        --source=!: The source of data used in the request to apply the guardrail.
      completion:
        flag:
            output-scope:
                - INTERVENTIONS
                - FULL
            source:
                - INPUT
                - OUTPUT
      documentation:
        command: |-
            The action to apply a guardrail.

            For troubleshooting some of the common errors you might encounter when using the `ApplyGuardrail` API, see [Troubleshooting Amazon Bedrock API Error Codes](https://docs.aws.amazon.com/bedrock/latest/userguide/troubleshooting-api-error-codes.html) in the Amazon Bedrock User Guide
        flag:
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            content: The content details used in the request to apply the guardrail.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            guardrail-identifier: The guardrail identifier used in the request to apply the guardrail.
            guardrail-version: The guardrail version used in the request to apply the guardrail.
            output-scope: |-
                Specifies the scope of the output that you get in the response. Set to `FULL` to return the entire output, including any detected and non-detected entries in the response for enhanced debugging.

                Note that the full output scope doesn't apply to word filters or regex in sensitive information filters. It does apply to all other filtering policies, including sensitive information with filters that can detect personally identifiable information (PII).
            source: The source of data used in the request to apply the guardrail.
    - name: converse
      description: Sends messages to the specified Amazon Bedrock model.
      flags:
        --additional-model-request-fields=: Additional inference parameters that the model supports, beyond the base set of inference parameters that `Converse` and `ConverseStream` support in the `inferenceConfig` field.
        --additional-model-response-field-paths=: Additional model parameters field paths to return in the response.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --guardrail-config=: Configuration information for a guardrail that you want to use in the request.
        --inference-config=: Inference parameters to pass to the model.
        --messages=: The messages that you want to send to the model.
        --model-id=!: Specifies the model or throughput with which to run inference, or the prompt resource to use in inference.
        --performance-config=: Model performance settings for the request.
        --prompt-variables=: Contains a map of variables in a prompt from Prompt management to objects containing the values to fill in for them when running model invocation.
        --request-metadata=: Key-value pairs that you can use to filter invocation logs.
        --service-tier=: Specifies the processing tier configuration used for serving the request.
        --system=: A prompt that provides instructions or context to the model about the task it should perform, or the persona it should adopt during the conversation.
        --tool-config=: Configuration information for the tools that the model can use when generating a response.
      documentation:
        command: |-
            Sends messages to the specified Amazon Bedrock model. `Converse` provides a consistent interface that works with all models that support messages. This allows you to write code once and use it with different models. If a model has unique inference parameters, you can also pass those unique parameters to the model.

            Amazon Bedrock doesn't store any text, images, or documents that you provide as content. The data is only used to generate the response.

            You can submit a prompt by including it in the `messages` field, specifying the `modelId` of a foundation model or inference profile to run inference on it, and including any other fields that are relevant to your use case.

            You can also submit a prompt from Prompt management by specifying the ARN of the prompt version and including a map of variables to values in the `promptVariables` field. You can append more messages to the prompt by using the `messages` field. If you use a prompt from Prompt management, you can't include the following fields in the request: `additionalModelRequestFields`, `inferenceConfig`, `system`, or `toolConfig`. Instead, these fields must be defined through Prompt management. For more information, see [Use a prompt from Prompt management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management-use.html).

            For information about the Converse API, see *Use the Converse API* in the *Amazon Bedrock User Guide*. To use a guardrail, see *Use a guardrail with the Converse API* in the *Amazon Bedrock User Guide*. To use a tool with a model, see *Tool use (Function calling)* in the *Amazon Bedrock User Guide*

            For example code, see *Converse API examples* in the *Amazon Bedrock User Guide*.

            This operation requires permission for the `bedrock:InvokeModel` action.

            To deny all inference access to resources that you specify in the modelId field, you need to deny access to the `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` actions. Doing this also denies access to the resource through the base inference actions ([InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html)). For more information see [Deny access for inference on specific models](https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-deny-inference).

            For troubleshooting some of the common errors you might encounter when using the `Converse` API, see [Troubleshooting Amazon Bedrock API Error Codes](https://docs.aws.amazon.com/bedrock/latest/userguide/troubleshooting-api-error-codes.html) in the Amazon Bedrock User Guide
        flag:
            additional-model-request-fields: Additional inference parameters that the model supports, beyond the base set of inference parameters that `Converse` and `ConverseStream` support in the `inferenceConfig` field. For more information, see [Model parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
            additional-model-response-field-paths: |-
                Additional model parameters field paths to return in the response. `Converse` and `ConverseStream` return the requested fields as a JSON Pointer object in the `additionalModelResponseFields` field. The following is example JSON for `additionalModelResponseFieldPaths`.

                `[ "/stop_sequence" ]`

                For information about the JSON Pointer syntax, see the [Internet Engineering Task Force (IETF)](https://datatracker.ietf.org/doc/html/rfc6901) documentation.

                `Converse` and `ConverseStream` reject an empty JSON Pointer or incorrectly structured JSON Pointer with a `400` error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by `Converse`.
            cli-input-json: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            cli-input-yaml: |-
                Reads arguments from the JSON string provided.
                The JSON string follows the  format  provided  by --generate-cli-skeleton.
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values.
                It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            generate-cli-skeleton: |-
                If other arguments are provided on the command line,  those  values  will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally.
                This may  not  be  specified  along with --cli-input-yaml.
            guardrail-config: Configuration information for a guardrail that you want to use in the request. If you include `guardContent` blocks in the `content` field in the `messages` field, the guardrail operates only on those messages. If you include no `guardContent` blocks, the guardrail operates on all messages in the request body and in any included prompt resource.
            inference-config: Inference parameters to pass to the model. `Converse` and `ConverseStream` support a base set of inference parameters. If you need to pass additional parameters that the model supports, use the `additionalModelRequestFields` request field.
            messages: The messages that you want to send to the model.
            model-id: |-
                Specifies the model or throughput with which to run inference, or the prompt resource to use in inference. The value depends on the resource that you use:

                - If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see [Amazon Bedrock base model IDs (on-demand throughput)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns) in the Amazon Bedrock User Guide.
                - If you use an inference profile, specify the inference profile ID or its ARN. For a list of inference profile IDs, see [Supported Regions and models for cross-region inference](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference-support.html) in the Amazon Bedrock User Guide.
                - If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see [Run inference using a Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html) in the Amazon Bedrock User Guide.
                - If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see [Use a custom model in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html) in the Amazon Bedrock User Guide.
                - To include a prompt that was defined in [Prompt management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html), specify the ARN of the prompt version to use.

                The Converse API doesn't support [imported models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).
            performance-config: Model performance settings for the request.
            prompt-variables: Contains a map of variables in a prompt from Prompt management to objects containing the values to fill in for them when running model invocation. This field is ignored if you don't specify a prompt resource in the `modelId` field.
            request-metadata: Key-value pairs that you can use to filter invocation logs.
            service-tier: Specifies the processing tier configuration used for serving the request.
            system: A prompt that provides instructions or context to the model about the task it should perform, or the persona it should adopt during the conversation.
            tool-config: |-
                Configuration information for the tools that the model can use when generating a response.

                For information about models that support tool use, see [Supported models and model features](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).
documentation:
    command: Describes the API operations for running inference using Amazon Bedrock models.
