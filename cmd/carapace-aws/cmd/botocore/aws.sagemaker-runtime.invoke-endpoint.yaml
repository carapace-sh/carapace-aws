name: invoke-endpoint
description: After you deploy a model into production using Amazon SageMaker AI hosting services, your client applications use this API to get inferences from the model hosted at the specified endpoint.
flags:
    --accept=: The desired MIME type of the inference response from the model container.
    --body=!: Provides input data, in the format specified in the `ContentType` request header.
    --cli-input-json=: Read arguments from the JSON string provided.
    --cli-input-yaml=: Read arguments from the YAML string provided.
    --content-type=: The MIME type of the input data in the request body.
    --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
    --enable-explanations=: An optional JMESPath expression used to override the `EnableExplanations` parameter of the `ClarifyExplainerConfig` API.
    --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
    --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
    --inference-component-name=: If the endpoint hosts one or more inference components, this parameter specifies the name of inference component to invoke.
    --inference-id=: If you provide a value, it is added to the captured data when you enable data capture on the endpoint.
    --session-id=: Creates a stateful session or identifies an existing one.
    --target-container-hostname=: If the endpoint hosts multiple containers and is configured to use direct invocation, this parameter specifies the host name of the container to invoke.
    --target-model=: The model to request for inference when invoking a multi-model endpoint.
    --target-variant=: Specify the production variant to send the inference request to when invoking an endpoint that is running two or more variants.
