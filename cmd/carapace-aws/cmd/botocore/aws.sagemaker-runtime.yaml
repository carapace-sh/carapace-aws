name: sagemaker-runtime
description: Amazon SageMaker Runtime
commands:
    - name: invoke-endpoint
      description: After you deploy a model into production using Amazon SageMaker AI hosting services, your client applications use this API to get inferences from the model hosted at the specified endpoint.
      flags:
        --accept=: The desired MIME type of the inference response from the model container.
        --body=!: Provides input data, in the format specified in the `ContentType` request header.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content-type=: The MIME type of the input data in the request body.
        --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
        --enable-explanations=: An optional JMESPath expression used to override the `EnableExplanations` parameter of the `ClarifyExplainerConfig` API.
        --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --inference-component-name=: If the endpoint hosts one or more inference components, this parameter specifies the name of inference component to invoke.
        --inference-id=: If you provide a value, it is added to the captured data when you enable data capture on the endpoint.
        --session-id=: Creates a stateful session or identifies an existing one.
        --target-container-hostname=: If the endpoint hosts multiple containers and is configured to use direct invocation, this parameter specifies the host name of the container to invoke.
        --target-model=: The model to request for inference when invoking a multi-model endpoint.
        --target-variant=: Specify the production variant to send the inference request to when invoking an endpoint that is running two or more variants.
    - name: invoke-endpoint-async
      description: After you deploy a model into production using Amazon SageMaker AI hosting services, your client applications use this API to get inferences from the model hosted at the specified endpoint in an asynchronous manner.
      flags:
        --accept=: The desired MIME type of the inference response from the model container.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content-type=: The MIME type of the input data in the request body.
        --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
        --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --inference-id=: The identifier for the inference request.
        --input-location=!: The Amazon S3 URI where the inference request payload is stored.
        --invocation-timeout-seconds=: Maximum amount of time in seconds a request can be processed before it is marked as expired.
        --request-ttl-seconds=: Maximum age in seconds a request can be in the queue before it is marked as expired.
    - name: invoke-endpoint-with-response-stream
      description: Invokes a model at the specified endpoint to return the inference response as a stream.
      flags:
        --accept=: The desired MIME type of the inference response from the model container.
        --body=!: Provides input data, in the format specified in the `ContentType` request header.
        --cli-input-json=: Read arguments from the JSON string provided.
        --cli-input-yaml=: Read arguments from the YAML string provided.
        --content-type=: The MIME type of the input data in the request body.
        --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
        --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
        --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
        --inference-component-name=: If the endpoint hosts one or more inference components, this parameter specifies the name of inference component to invoke for a streaming response.
        --inference-id=: An identifier that you assign to your request.
        --session-id=: The ID of a stateful session to handle your request.
        --target-container-hostname=: If the endpoint hosts multiple containers and is configured to use direct invocation, this parameter specifies the host name of the container to invoke.
        --target-variant=: Specify the production variant to send the inference request to when invoking an endpoint that is running two or more variants.
