name: invoke-endpoint-with-response-stream
description: Invokes a model at the specified endpoint to return the inference response as a stream.
flags:
    --accept=: The desired MIME type of the inference response from the model container.
    --body=!: Provides input data, in the format specified in the `ContentType` request header.
    --cli-input-json=: Read arguments from the JSON string provided
    --cli-input-yaml=: Read arguments from the YAML string provided
    --content-type=: The MIME type of the input data in the request body.
    --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
    --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
    --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request
    --inference-component-name=: If the endpoint hosts one or more inference components, this parameter specifies the name of inference component to invoke for a streaming response.
    --inference-id=: An identifier that you assign to your request.
    --session-id=: The ID of a stateful session to handle your request.
    --target-container-hostname=: If the endpoint hosts multiple containers and is configured to use direct invocation, this parameter specifies the host name of the container to invoke.
    --target-variant=: Specify the production variant to send the inference request to when invoking an endpoint that is running two or more variants.
