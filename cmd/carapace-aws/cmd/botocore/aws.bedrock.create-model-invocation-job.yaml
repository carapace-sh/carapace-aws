name: create-model-invocation-job
description: Creates a batch inference job to invoke a model on multiple prompts.
flags:
    --client-request-token=: A unique, case-sensitive identifier to ensure that the API request completes no more than one time.
    --input-data-config=!: Details about the location of the input to the batch inference job.
    --job-name=!: A name to give the batch inference job.
    --model-id=!: The unique identifier of the foundation model to use for the batch inference job.
    --output-data-config=!: Details about the location of the output of the batch inference job.
    --role-arn=!: The Amazon Resource Name (ARN) of the service role with permissions to carry out and manage batch inference.
    --tags=: Any tags to associate with the batch inference job.
    --timeout-duration-in-hours=: The number of hours after which to force the batch inference job to time out.
    --vpc-config=: The configuration of the Virtual Private Cloud (VPC) for the data in the batch inference job.
