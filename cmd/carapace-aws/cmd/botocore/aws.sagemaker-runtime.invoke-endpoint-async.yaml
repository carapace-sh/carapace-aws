name: invoke-endpoint-async
description: After you deploy a model into production using Amazon SageMaker AI hosting services, your client applications use this API to get inferences from the model hosted at the specified endpoint in an asynchronous manner.
flags:
    --accept=: The desired MIME type of the inference response from the model container.
    --cli-input-json=: Read arguments from the JSON string provided
    --cli-input-yaml=: Read arguments from the YAML string provided
    --content-type=: The MIME type of the input data in the request body.
    --custom-attributes=: Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint.
    --endpoint-name=!: The name of the endpoint that you specified when you created the endpoint using the [CreateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html) API.
    --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request
    --inference-id=: The identifier for the inference request.
    --input-location=!: The Amazon S3 URI where the inference request payload is stored.
    --invocation-timeout-seconds=: Maximum amount of time in seconds a request can be processed before it is marked as expired.
    --request-ttl-seconds=: Maximum age in seconds a request can be in the queue before it is marked as expired.
