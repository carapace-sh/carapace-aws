name: converse-stream
description: Sends messages to the specified Amazon Bedrock model and returns the response in a stream.
flags:
    --additional-model-request-fields=: Additional inference parameters that the model supports, beyond the base set of inference parameters that `Converse` and `ConverseStream` support in the `inferenceConfig` field.
    --additional-model-response-field-paths=: Additional model parameters field paths to return in the response.
    --cli-input-json=: Read arguments from the JSON string provided.
    --cli-input-yaml=: Read arguments from the YAML string provided.
    --generate-cli-skeleton: Prints a JSON skeleton to standard output without sending an API request.
    --guardrail-config=: Configuration information for a guardrail that you want to use in the request.
    --inference-config=: Inference parameters to pass to the model.
    --messages=: The messages that you want to send to the model.
    --model-id=!: Specifies the model or throughput with which to run inference, or the prompt resource to use in inference.
    --performance-config=: Model performance settings for the request.
    --prompt-variables=: Contains a map of variables in a prompt from Prompt management to objects containing the values to fill in for them when running model invocation.
    --request-metadata=: Key-value pairs that you can use to filter invocation logs.
    --system=: A prompt that provides instructions or context to the model about the task it should perform, or the persona it should adopt during the conversation.
    --tool-config=: Configuration information for the tools that the model can use when generating a response.
